{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "middle-baltimore",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests as rq\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "options = Options()\n",
    "options.headless = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "secure-ready",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 28.304211378097534 seconds ---\n"
     ]
    }
   ],
   "source": [
    "def func_get_url_list(url_name):        # appropriate to pass element\\n\",\n",
    "    url_set = set()\n",
    "    # browser.get('https://www.goodreads.com/list/show/50.The_Best_Epic_Fantasy_fiction_')\n",
    "    browser = webdriver.Firefox(options=options, executable_path=\"./drivers/geckodriver\")    \n",
    "    browser.get(url_name)\n",
    "    element2 = browser.find_elements_by_tag_name('a')\n",
    "    # print(len(element2))\n",
    "    for elem in element2:\n",
    "        href = elem.get_attribute('href')\n",
    "        substring  = \"https://www.goodreads.com/book/show/\"\n",
    "        if href is not None:\n",
    "            if substring in href:\n",
    "                url_set.add(href)\n",
    "    browser.close()\n",
    "    #print(url_set)\n",
    "    return url_set\n",
    "\n",
    "start_time = time.time()\n",
    "url = \"https://www.goodreads.com/list/show/50.The_Best_Epic_Fantasy_fiction_\"\n",
    "a = func_get_url_list(url)         # slow\n",
    "print(\"time taken: %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "funny-deposit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link 0: https://www.goodreads.com/list/show/50.The_Best_Epic_Fantasy_fiction_?page=1\n",
      "link 1: https://www.goodreads.com/list/show/50.The_Best_Epic_Fantasy_fiction_?page=2\n",
      "link 2: https://www.goodreads.com/list/show/50.The_Best_Epic_Fantasy_fiction_?page=3\n",
      "                                                   url\n",
      "0    https://www.goodreads.com//book/show/13496.A_G...\n",
      "1    https://www.goodreads.com//book/show/30.J_R_R_...\n",
      "2    https://www.goodreads.com//book/show/186074.Th...\n",
      "3    https://www.goodreads.com//book/show/7235533-t...\n",
      "4    https://www.goodreads.com//book/show/11127.The...\n",
      "..                                                 ...\n",
      "295  https://www.goodreads.com//book/show/34484.Sma...\n",
      "296  https://www.goodreads.com//book/show/43919.Fae...\n",
      "297  https://www.goodreads.com//book/show/13624.The...\n",
      "298  https://www.goodreads.com//book/show/45101.The...\n",
      "299  https://www.goodreads.com//book/show/64105.The...\n",
      "\n",
      "[300 rows x 1 columns]\n",
      "time taken: 19.44176745414734 seconds ---\n"
     ]
    }
   ],
   "source": [
    "base_url = 'https://www.goodreads.com/list/show/50.The_Best_Epic_Fantasy_fiction_'\n",
    "\n",
    "def get_books_url_per_page(base_url, tag, tag_class):\n",
    "\n",
    "    book_url_list = []\n",
    "    base_page = rq.get(base_url)\n",
    "    base_content = BeautifulSoup(base_page.content, 'html.parser')\n",
    "\n",
    "    for a in base_content.find_all(tag, class_= tag_class):\n",
    "        book_url_list.append('https://www.goodreads.com/' + a['href'])\n",
    "    \n",
    "    return book_url_list\n",
    "\n",
    "\n",
    "def get_base_page_list(base_url, pages):\n",
    "    base_page_list = []\n",
    "    for i in range(pages):\n",
    "        base_page_list.append(base_url + str(i + 1))\n",
    "    \n",
    "    return base_page_list\n",
    "\n",
    "\n",
    "def get_whole_book_links(base_url, pages):\n",
    "    whole_book_url_list = []\n",
    "    human_like_time = np.random.uniform(2, 7)\n",
    "    for i, link in enumerate(get_base_page_list(base_url, pages)):\n",
    "        whole_book_url_list += get_books_url_per_page(link, 'a', 'bookTitle')\n",
    "        print(f'link {i}: {link}')\n",
    "\n",
    "    # whole_book_url_list = [link for subs in whole_book_url_list for link in subs]\n",
    "    return whole_book_url_list\n",
    "\n",
    "browse_page = 3 \n",
    "start_time = time.time()\n",
    "\n",
    "whole_book_url_list_ = get_whole_book_links('https://www.goodreads.com/list/show/50.The_Best_Epic_Fantasy_fiction_?page=', browse_page)\n",
    "\n",
    "link_df = pd.DataFrame({'url': whole_book_url_list_})\n",
    "print(link_df)\n",
    "#link_df.to_csv('./links_temp.csv')\n",
    "print(\"time taken: %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "# analyse task2\n",
    "def func_mean_minmax_norm_ratings(authorname, dataframe):\n",
    "    f=df2[df2.loc[:,'author'] == 'J.K. Rowling']\n",
    "    desired_book = f[f.loc[:,'minmax_norm_ratings']== f['minmax_norm_ratings'].max()]\n",
    "    return desired_book.title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-burden",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper():\n",
    "    myDict1 = {}\n",
    "    start_time = time.time()\n",
    "\n",
    "    for page_num in range(0, 2):\n",
    "        print(page_num,end = \" \")\n",
    "        browser = webdriver.Firefox( executable_path=\"./drivers/geckodriver\")\n",
    "        browser.get(url_list[page_num])\n",
    "        browser.find_element_by_partial_link_text(\"More Details...\").click()\n",
    "        title = browser.find_element_by_id('bookTitle').text\n",
    "        author = browser.find_element_by_class_name(\"authorName\").text \n",
    "        avg_rating = browser.find_element_by_xpath(\"/html/body/div[2]/div[3]/div[1]/div[2]/div[4]/div[1]/div[2]/div[2]/span[2]\").text\n",
    "        num_rating = browser.find_element_by_xpath(\"/html/body/div[2]/div[3]/div[1]/div[2]/div[4]/div[1]/div[2]/div[2]/a[2]\").text\n",
    "        num_review = browser.find_element_by_xpath(\"/html/body/div[2]/div[3]/div[1]/div[2]/div[4]/div[1]/div[2]/div[2]/a[3]\").text\n",
    "        details = browser.find_element_by_id(\"details\")\n",
    "        temp1 = details.text.split(\"\\n\")\n",
    "\n",
    "        page = 0\n",
    "        series = 0\n",
    "\n",
    "        # num rating\n",
    "        try:\n",
    "            num_ratings = num_rating.split()[0]\n",
    "        except:\n",
    "            num_ratings = 0\n",
    "\n",
    "        # num review\n",
    "        try:\n",
    "            num_reviews = num_review.split()[0]\n",
    "        except:\n",
    "            num_reviews = 0\n",
    "\n",
    "        places = np.nan\n",
    "        awards = np.nan\n",
    "\n",
    "        # total_pages\n",
    "        try:\n",
    "            num_pages = [int(s) for s in temp1[0].split() if s.isdigit()]\n",
    "            num_pages = str(num_pages[0])\n",
    "        except IndexError:\n",
    "            num_pages = np.nan\n",
    "\n",
    "        # year\n",
    "        try:\n",
    "            year = [int(s) for s in temp1[1].split() if s.isdigit()]\n",
    "            original_publish_year = str(year[0]) \n",
    "        except IndexError:\n",
    "            original_publish_year = np.nan\n",
    "\n",
    "        # places\n",
    "        for i in range(len(temp1)):\n",
    "            if(temp1[i]==\"Setting\"):\n",
    "                places = temp1[i+1]\n",
    "\n",
    "        # series\n",
    "        for i in range(len(temp1)):\n",
    "            if(temp1[i] == \"Series\"):\n",
    "                series = 1\n",
    "\n",
    "        # Awards\n",
    "        for i in range(len(temp1)):\n",
    "            if(temp1[i] == \"Literary Awards\"):\n",
    "                awards = temp1[i+1]\n",
    "\n",
    "        # genre\n",
    "        genre = browser.find_elements_by_css_selector(\"div.left>a.bookPageGenreLink\")\n",
    "        temp2 = [i.text for i in genre]\n",
    "\n",
    "        try:\n",
    "            genres = temp2[0:3]\n",
    "        except IndexError:\n",
    "            genres = np.nan\n",
    "\n",
    "        myDict1[page_num] = (url_list[page_num],title, author, avg_rating, num_ratings,  num_reviews,  num_pages, original_publish_year, places, series, awards, genres)\n",
    "        browser.close()\n",
    "\n",
    "    print(\"time taken: %s seconds ---\" % (time.time() - start_time))\n",
    "    df = pd.DataFrame.from_dict(myDict1, orient='index', columns=['url','title', 'author', 'num_reviews', 'num_ratings', 'avg_rating',  'num_pages', 'original_publish_year', 'series', 'genres', 'awards', 'places'])\n",
    "    dfn = df.convert_dtypes()\n",
    "    dfn['series'] = dfn['series'].astype('bool')\n",
    "    dfn.to_csv('filename.csv')\n",
    "    \n",
    "def preprocessing(csv_path):\n",
    "    df = pd.read_csv('final_01.csv')  # csv_path\n",
    "    #df2['awards'].isna().any()\n",
    "    df5 = df \n",
    "    df5['Awards_count'] = df5.awards.str.count(',')+1\n",
    "    df5['Awards_count'] = df5['Awards_count'].fillna(value=0) \n",
    "    \n",
    "    # min-max\n",
    "    df1 = df5\n",
    "    max_min_value = df['avg_rating'].max() - df['avg_rating'].min()\n",
    "    min_value = df['avg_rating'].min()\n",
    "    df1['minmax_norm_ratings'] = 1 + (df1['avg_rating'] - min_value) / (max_min_value) * 9\n",
    "    \n",
    "    # mean\n",
    "    max_min_value = df1['avg_rating'].max() - df1['avg_rating'].min()\n",
    "    mean_value = df1['avg_rating'].mean()\n",
    "    df1['mean_norm_ratings'] = 1 + (df1['avg_rating'] - mean_value) / (max_min_value) * 9\n",
    "    return df1\n",
    "\n",
    "def analyse(df):\n",
    "    # task 1\n",
    "    df2 = df\n",
    "    #df2.dropna(inplace=True)\n",
    "    df2['original_publish_year'].dropna(inplace=True) \n",
    "    df2['original_publish_year'].nunique()\n",
    "    min_1 = df2['original_publish_year'].quantile(0.01)\n",
    "    min_2 = df2['original_publish_year'].quantile(0.99)\n",
    "    df2['original_publish_year'] = np.where(df2['original_publish_year'] < min_1, min_1,df2['original_publish_year'])\n",
    "    df2['original_publish_year'] = np.where(df2['original_publish_year'] > min_2, min_2,df2['original_publish_year'])\n",
    "    s = df2.groupby(['original_publish_year'])['minmax_norm_ratings'].mean()\n",
    "    df3 = s.to_frame()\n",
    "    df3.plot()  # .get_figure()\n",
    "    \n",
    "    # task 2\n",
    "    bookname = func_mean_minmax_norm_ratings('J.K. Rowling',df2)          # function above\n",
    "    \n",
    "def main():\n",
    "    scraper()\n",
    "    df = preprocessing(csv_path)\n",
    "    analyse(df)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\tmain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-queen",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
