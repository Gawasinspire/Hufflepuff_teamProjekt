{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "middle-baltimore",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests as rq\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "options = Options()\n",
    "options.headless = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "funny-deposit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3d6a133acf0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mwhole_book_url_list_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_whole_book_links\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://www.goodreads.com/list/show/50.The_Best_Epic_Fantasy_fiction_?page='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mlink_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwhole_book_url_list_\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m#link_df.to_csv('./links_temp.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/StriveSchool/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/StriveSchool/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         ]\n\u001b[0;32m--> 287\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/StriveSchool/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/StriveSchool/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mindexes\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mraw_lengths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"If using all scalar values, you must pass an index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhave_series\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "\n",
    "base_url = 'https://www.goodreads.com/list/show/50.The_Best_Epic_Fantasy_fiction_'\n",
    "\n",
    "def get_books_url_per_page(base_url, tag, tag_class):\n",
    "\n",
    "    book_url_list = []\n",
    "    base_page = rq.get(base_url)\n",
    "    base_content = BeautifulSoup(base_page.content, 'html.parser')\n",
    "\n",
    "    for a in base_content.find_all(tag, class_= tag_class):\n",
    "        book_url_list.append('https://www.goodreads.com/' + a['href'])\n",
    "    \n",
    "    return book_url_list\n",
    "\n",
    "\n",
    "def get_base_page_list(base_url, pages):\n",
    "    base_page_list = []\n",
    "    for i in range(pages):\n",
    "        base_page_list.append(base_url + str(i + 1))\n",
    "    \n",
    "    return base_page_list\n",
    "\n",
    "\n",
    "def get_whole_book_links(base_url, pages):\n",
    "    whole_book_url_list = []\n",
    "\n",
    "    for link in get_base_page_list(base_url, pages):\n",
    "        whole_book_url_list.append(get_books_url_per_page(link, 'a', 'bookTitle'))\n",
    "\n",
    "    whole_book_url_list = [link for subs in whole_book_url_list for link in subs]\n",
    "\n",
    "    print(len(whole_book_url_list))\n",
    "#browse_page = 3 #36\n",
    "whole_book_url_list_ = get_whole_book_links('https://www.goodreads.com/list/show/50.The_Best_Epic_Fantasy_fiction_?page=', 3)\n",
    "\n",
    "link_df = pd.DataFrame({'url': whole_book_url_list_})\n",
    "print(link_df)\n",
    "#link_df.to_csv('./links_temp.csv')\n",
    "\n",
    "# analyse task2\n",
    "def func_mean_minmax_norm_ratings(authorname, dataframe):\n",
    "    f=df2[df2.loc[:,'author'] == 'J.K. Rowling']\n",
    "    desired_book = f[f.loc[:,'minmax_norm_ratings']== f['minmax_norm_ratings'].max()]\n",
    "    return desired_book.title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-burden",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper():\n",
    "    myDict1 = {}\n",
    "    start_time = time.time()\n",
    "\n",
    "    for page_num in range(0, 2):\n",
    "        print(page_num,end = \" \")\n",
    "        browser = webdriver.Firefox( executable_path=\"./drivers/geckodriver\")\n",
    "        browser.get(url_list[page_num])\n",
    "        browser.find_element_by_partial_link_text(\"More Details...\").click()\n",
    "        title = browser.find_element_by_id('bookTitle').text\n",
    "        author = browser.find_element_by_class_name(\"authorName\").text \n",
    "        avg_rating = browser.find_element_by_xpath(\"/html/body/div[2]/div[3]/div[1]/div[2]/div[4]/div[1]/div[2]/div[2]/span[2]\").text\n",
    "        num_rating = browser.find_element_by_xpath(\"/html/body/div[2]/div[3]/div[1]/div[2]/div[4]/div[1]/div[2]/div[2]/a[2]\").text\n",
    "        num_review = browser.find_element_by_xpath(\"/html/body/div[2]/div[3]/div[1]/div[2]/div[4]/div[1]/div[2]/div[2]/a[3]\").text\n",
    "        details = browser.find_element_by_id(\"details\")\n",
    "        temp1 = details.text.split(\"\\n\")\n",
    "\n",
    "        page = 0\n",
    "        series = 0\n",
    "\n",
    "        # num rating\n",
    "        try:\n",
    "            num_ratings = num_rating.split()[0]\n",
    "        except:\n",
    "            num_ratings = 0\n",
    "\n",
    "        # num review\n",
    "        try:\n",
    "            num_reviews = num_review.split()[0]\n",
    "        except:\n",
    "            num_reviews = 0\n",
    "\n",
    "        places = np.nan\n",
    "        awards = np.nan\n",
    "\n",
    "        # total_pages\n",
    "        try:\n",
    "            num_pages = [int(s) for s in temp1[0].split() if s.isdigit()]\n",
    "            num_pages = str(num_pages[0])\n",
    "        except IndexError:\n",
    "            num_pages = np.nan\n",
    "\n",
    "        # year\n",
    "        try:\n",
    "            year = [int(s) for s in temp1[1].split() if s.isdigit()]\n",
    "            original_publish_year = str(year[0]) \n",
    "        except IndexError:\n",
    "            original_publish_year = np.nan\n",
    "\n",
    "        # places\n",
    "        for i in range(len(temp1)):\n",
    "            if(temp1[i]==\"Setting\"):\n",
    "                places = temp1[i+1]\n",
    "\n",
    "        # series\n",
    "        for i in range(len(temp1)):\n",
    "            if(temp1[i] == \"Series\"):\n",
    "                series = 1\n",
    "\n",
    "        # Awards\n",
    "        for i in range(len(temp1)):\n",
    "            if(temp1[i] == \"Literary Awards\"):\n",
    "                awards = temp1[i+1]\n",
    "\n",
    "        # genre\n",
    "        genre = browser.find_elements_by_css_selector(\"div.left>a.bookPageGenreLink\")\n",
    "        temp2 = [i.text for i in genre]\n",
    "\n",
    "        try:\n",
    "            genres = temp2[0:3]\n",
    "        except IndexError:\n",
    "            genres = np.nan\n",
    "\n",
    "        myDict1[page_num] = (url_list[page_num],title, author, avg_rating, num_ratings,  num_reviews,  num_pages, original_publish_year, places, series, awards, genres)\n",
    "        browser.close()\n",
    "\n",
    "    print(\"time taken: %s seconds ---\" % (time.time() - start_time))\n",
    "    df = pd.DataFrame.from_dict(myDict1, orient='index', columns=['url','title', 'author', 'num_reviews', 'num_ratings', 'avg_rating',  'num_pages', 'original_publish_year', 'series', 'genres', 'awards', 'places'])\n",
    "    dfn = df.convert_dtypes()\n",
    "    dfn['series'] = dfn['series'].astype('bool')\n",
    "    dfn.to_csv('filename.csv')\n",
    "    \n",
    "def preprocessing(csv_path):\n",
    "    df = pd.read_csv('final_01.csv')  # csv_path\n",
    "    #df2['awards'].isna().any()\n",
    "    df5 = df \n",
    "    df5['Awards_count'] = df5.awards.str.count(',')+1\n",
    "    df5['Awards_count'] = df5['Awards_count'].fillna(value=0) \n",
    "    \n",
    "    # min-max\n",
    "    df1 = df5\n",
    "    max_min_value = df['avg_rating'].max() - df['avg_rating'].min()\n",
    "    min_value = df['avg_rating'].min()\n",
    "    df1['minmax_norm_ratings'] = 1 + (df1['avg_rating'] - min_value) / (max_min_value) * 9\n",
    "    \n",
    "    # mean\n",
    "    max_min_value = df1['avg_rating'].max() - df1['avg_rating'].min()\n",
    "    mean_value = df1['avg_rating'].mean()\n",
    "    df1['mean_norm_ratings'] = 1 + (df1['avg_rating'] - mean_value) / (max_min_value) * 9\n",
    "    return df1\n",
    "\n",
    "def analyse(df):\n",
    "    # task 1\n",
    "    df2 = df\n",
    "    #df2.dropna(inplace=True)\n",
    "    df2['original_publish_year'].dropna(inplace=True) \n",
    "    df2['original_publish_year'].nunique()\n",
    "    min_1 = df2['original_publish_year'].quantile(0.01)\n",
    "    min_2 = df2['original_publish_year'].quantile(0.99)\n",
    "    df2['original_publish_year'] = np.where(df2['original_publish_year'] < min_1, min_1,df2['original_publish_year'])\n",
    "    df2['original_publish_year'] = np.where(df2['original_publish_year'] > min_2, min_2,df2['original_publish_year'])\n",
    "    s = df2.groupby(['original_publish_year'])['minmax_norm_ratings'].mean()\n",
    "    df3 = s.to_frame()\n",
    "    df3.plot()  # .get_figure()\n",
    "    \n",
    "    # task 2\n",
    "    bookname = func_mean_minmax_norm_ratings('J.K. Rowling',df2)          # function above\n",
    "    \n",
    "def main():\n",
    "    scraper()\n",
    "    df = preprocessing(csv_path)\n",
    "    analyse(df)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\tmain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-queen",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
